{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Ricky Truong\n",
    "#### Student ID: s3783560\n",
    "\n",
    "Date: XXXX\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* re\n",
    "* numpy\n",
    "* itertools\n",
    "* os\n",
    "* nltk\n",
    "\n",
    "## Introduction\n",
    "This this notebook we are provided with hundreds of text files each corresponding to a different classes, that are 'Accounting Finance', 'Engineering', 'Healthcare Nursing' or 'Sales'. Our goal is to apply basic text pro-processings steps to an job description that is extract from each file. Each file contains the following: a title, a web index, a job description and some may also have a company name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricky\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first inspect the file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']\n"
     ]
    }
   ],
   "source": [
    "# Code to inspect the provided data file...\n",
    "files = os.listdir(\"data\") #get list of files in the \"data\" folder\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"data/Accounting_Finance\") # get list of all files in 'Accounting_Finance' folder\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"data/Engineering/\") # get list of all files in Engineering folder\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"data/Healthcare_Nursing/\") # get list of all files in Healthcare_Nursing folder\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(\"data/Sales/\")# get list of all files in Sales folder\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title: Assistant Branch Manager\\n',\n",
       " 'Webindex: 69598361\\n',\n",
       " 'Company: Ably Resources Ltd\\n',\n",
       " 'Description: Assistant Branch Manager Midlands THE CLIENT: This market leading Service Provider has grown significantly over the past few years. Operating on a local and regional basis, they continue to provide an innovative service to all Commercial Public sector clients. Due to continual growth, they now seek an Assistant Branch Manager to assist with the Midlands operations. THE ROLE: You will be responsible for a large team of Sales, Operations Admin staff, based from Birmingham and covering the Midlands/South West area. You will be involved in the general day to day running of the branch, management of staff and operations in addition to leading the team to achieve all set targets. The ideal candidate will have experience in managing upwards of **** employees in a dynamic, fast paced environment. Candidates must have a minimum of 5 years managerial experience and have a proactive and organised approach to the position. Applicants should note that this will be a challenging role and can be expected to work long hours to achieve all goals. THE PACKAGE: Basic Salary  ****  **** DOE Benefits  full benefits package Apply online or send your CV to Yvonne.crawfordablyresources.com Please note that only suitable candidates will be notified This job was originally posted as www.totaljobs.com/JobSeeking/AssistantBranchManager_job****']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first file in the sales folder\n",
    "with open(\"data/Sales/Job_00621.txt\", 'r') as f:\n",
    "    data = f.readlines()\n",
    "    f.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title: Water Hygiene Engineer\\n',\n",
       " 'Webindex: 72481581\\n',\n",
       " 'Company: Jobsite Jobs\\n',\n",
       " 'Description: Job Title: Water Hygiene Engineer Location Post Code: BS**** (Bristol, Avon) Salary: ****K to ****K (Depending on Skills Experience) Our client is looking for an experienced Water Hygiene Engineer to join their growing team in Bristol. The ideal candidate will have substantial field service experience preferably in the Water Hygiene Industry, an exceptional mechanical aptitude and great customer service skills. They are looking for enthusiastic people with experience in;  Cold water storage tank cleaning and disinfection, including handling and dosing chemicals  Cold water storage tank lining, including using an angle grinder and painting  Showerhead cleaning and disinfection  Cold water storage tank inspections  Water hygiene monitoring tasks including temperature testing, tank inspections, samples and calorifier inspections  Driving ****k miles per year You must have  GCSE Maths and English grade C and above  Experience in Water Hygiene / Legionella control industry  Mechanical aptitude and technical ability  Experience at working unsupervised  Capable of physical work, lifting, carrying and climbing  Full current UK driving Licence In return they are offering a fantastic opportunity to work for a great company, on a good salary, with a van, overtime and generous holiday package. The hours are Monday to Friday 8.30am to 5.30pm. The role is managed from their Avonmouth office, and the work is at their customers sites throughout the UK, with the majority of the work in the South West. About our client Our client is a Bristol based Water Hygiene Service Company. They currently employ **** people and due to continued growth they are looking for an experienced Engineer to join their team. They are specialists operating in the building services sector. They provide solutions for statutory Health and Safety compliance issues, such as the control and prevention of Legionnaires disease. They are signatories to the Legionella Control Associations Code of Conduct which is the recognised trade body for what they do. They are built on a vision of providing exceptional service and value for money to their customers. They achieve this by being passionate that their Engineers are the best trained, best equipped and best presented in their industry. Selection Process: Please apply online with your CV and covering letter explaining the reasons why your skills and experience would make you the best candidate for the job. Further details will be sent out to the email address specified in your application All selected candidates will be subject to a six month probation period. All employment is subject to a satisfactory Criminal records bureau check (CRB). Our client is an equal opportunities employer.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first file in the engineering folder\n",
    "with open(\"data/Engineering/Job_00017.txt\", 'r') as f:\n",
    "    data = f.readlines()\n",
    "    f.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Guinness Care and Support are looking for versatile, flexible individuals who are comfortable working out in the community, providing personal care and support to customers with a range of needs. We can provide full or part time hours and are looking for people who can be flexible to work shifts between 7am to 10pm, on a 7 day rota. The purpose of this role is to provide a high quality and customer focused care and support service that responds to the needs of our customers, enhances their quality of life and promotes independence. You will be able to demonstrate a caring and supportive attitude towards vulnerable people, be able to work a flexible shift pattern, have good communication skills, be able to work on your initiative and possess a good level of literacy and numeracy. A full driving licence and access to a vehicle is essential for this position. Mileage will be paid at ****p per mile, travel time paid at the normal hourly rate. Please note we do not accept CVs for our vacancies unless accompanied by a completed application form. Once we have received your application we will get in touch with shortlisted applicants within two weeks. If you do not hear from us please assume that your application hasn't been successful this time. Please accept our apologies for not being able to acknowledge all applications. Please note that an enhanced Criminal Records Bureau check will be required for this role and in return we offer excellent benefits, first class training and development and we are committed to equal opportunities. Closing date: 31st January 2013\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the first file in the engineering folder\n",
    "with open(\"data/Healthcare_Nursing/Job_00426.txt\", 'r') as f:\n",
    "    data = f.readlines()\n",
    "    f.close()\n",
    "data[-1][13:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code blocks, we can see that the data folder contains 4 folders not including the hidden folder. The folder names are 'Accounting_Finance', 'Engineering', 'Healthcare_Nursing', and 'Sales'. Each folder contains 191, 231, 198,  and 156 text files, respectively, for a total of 776 text files. Reading the text files, we can see that each text file contains the title of the job, a webindex, a description of the job and some of them, not all will also contain the company for which the job is for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we shall extract job description and some additional information from the text files as they will come in handy in the later tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"./data\"\n",
    "job_description = []\n",
    "job_index = []\n",
    "target_class = []\n",
    "title = []\n",
    "for folder in os.listdir(dir_path):# load all folders\n",
    "    if folder != '.DS_Store': #anything folder but the hidden folder '.DS_Store'\n",
    "        path = os.path.join(dir_path,folder) # e.g /data/Engineering\n",
    "        for filename in os.listdir(path): #load all text files\n",
    "            new_path = os.path.join(path,filename) # e.g /data/Engineering/Job_00017.txt\n",
    "            with open(new_path,\"r\",encoding = 'unicode_escape') as f:\n",
    "                data = (f.readlines())\n",
    "                title.append(data[0].strip()) #append title of each job to list\n",
    "                target_class.append(str(folder))#save the folder name as the class\n",
    "                job_index.append(data[1][10:]) #get the webindex from the digits onwards\n",
    "                \n",
    "                job_description.append(data[-1]) #data[-1] obtains the last lines which is the description blocks and appends to list\n",
    "                f.close()\n",
    "print(len(job_description)) # length of list should equal total number of files which is 776\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71614229\n",
      "\n",
      "Description: Our client one of the UK's leading Care Providers is looking to recruit an Assisted Living Coordinator for their facility in Westbourne.The Assisted Living Coordinator will have full responsibility for the day to day management of all the Assisted Living functions. The Assisted Living Coordinator reports to Registered Manager for all matters relating to regulatory standards set out by CQC. Responsibilities include:Resident Programmes and CareFinancial ManagementQuality Assurance and Regulatory People ManagementFamily ServicesThis facility is registered Nursing so you will need to be a RGN. If you can demonstrates a strong career path within elderly care and are looking for a position in the Westbourne area, please contact us ASAP for an early interview.\n"
     ]
    }
   ],
   "source": [
    "#randomly pick a job description and webindex to see if its right\n",
    "print(job_index[485])\n",
    "print(job_description[485])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully extracted the job description of all files and can now proceed with text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the assignment guidelines we will process the descriptions of the jobs in the following way.\n",
    "- Tokenisation of each job description string\n",
    "- Convert all words to lower case\n",
    "- Remove all words with less than 2 characters\n",
    "- Remove all stopwords, with the given stopwords file\n",
    "- Remove words that appear only once, based on term frequency\n",
    "- Remove the top 50 most common wors based on document frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function tokenise strings and converts all characters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniseData(description):\n",
    "    '''\n",
    "    Function tokenises a description string\n",
    "    '''\n",
    "    lower_description = description.lower() # convert all to lowercase\n",
    "    pattern = r'''(?x)\n",
    "    [a-zA-Z]+(?:[-'][a-zA-Z]+)? # whole words or words with hyphens/ apostrophe\n",
    "    '''\n",
    "    tokenizer = nltk.RegexpTokenizer(pattern) \n",
    "    tokenised_description = tokenizer.tokenize(lower_description)\n",
    "    return tokenised_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to tokenise every description in list\n",
    "tokenised_description = [tokeniseData(description) for description in job_description] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'our', 'client', 'one', 'of', 'the', \"uk's\", 'leading', 'care', 'providers', 'is', 'looking', 'to', 'recruit', 'an', 'assisted', 'living', 'coordinator', 'for', 'their', 'facility', 'in', 'westbourne', 'the', 'assisted', 'living', 'coordinator', 'will', 'have', 'full', 'responsibility', 'for', 'the', 'day', 'to', 'day', 'management', 'of', 'all', 'the', 'assisted', 'living', 'functions', 'the', 'assisted', 'living', 'coordinator', 'reports', 'to', 'registered', 'manager', 'for', 'all', 'matters', 'relating', 'to', 'regulatory', 'standards', 'set', 'out', 'by', 'cqc', 'responsibilities', 'include', 'resident', 'programmes', 'and', 'carefinancial', 'managementquality', 'assurance', 'and', 'regulatory', 'people', 'managementfamily', 'servicesthis', 'facility', 'is', 'registered', 'nursing', 'so', 'you', 'will', 'need', 'to', 'be', 'a', 'rgn', 'if', 'you', 'can', 'demonstrates', 'a', 'strong', 'career', 'path', 'within', 'elderly', 'care', 'and', 'are', 'looking', 'for', 'a', 'position', 'in', 'the', 'westbourne', 'area', 'please', 'contact', 'us', 'asap', 'for', 'an', 'early', 'interview']\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "#print to check results of random descprition \n",
    "print(tokenised_description[485])\n",
    "print(len(tokenised_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that all strings are tokenised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Removing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below removes all tokens with only one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'our', 'client', 'one', 'of', 'the', \"uk's\", 'leading', 'care', 'providers', 'is', 'looking', 'to', 'recruit', 'an', 'assisted', 'living', 'coordinator', 'for', 'their', 'facility', 'in', 'westbourne', 'the', 'assisted', 'living', 'coordinator', 'will', 'have', 'full', 'responsibility', 'for', 'the', 'day', 'to', 'day', 'management', 'of', 'all', 'the', 'assisted', 'living', 'functions', 'the', 'assisted', 'living', 'coordinator', 'reports', 'to', 'registered', 'manager', 'for', 'all', 'matters', 'relating', 'to', 'regulatory', 'standards', 'set', 'out', 'by', 'cqc', 'responsibilities', 'include', 'resident', 'programmes', 'and', 'carefinancial', 'managementquality', 'assurance', 'and', 'regulatory', 'people', 'managementfamily', 'servicesthis', 'facility', 'is', 'registered', 'nursing', 'so', 'you', 'will', 'need', 'to', 'be', 'rgn', 'if', 'you', 'can', 'demonstrates', 'strong', 'career', 'path', 'within', 'elderly', 'care', 'and', 'are', 'looking', 'for', 'position', 'in', 'the', 'westbourne', 'area', 'please', 'contact', 'us', 'asap', 'for', 'an', 'early', 'interview']\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "#for each description in the list only keep words longer or equal to 2 characters\n",
    "tokenised_description = [[w for w in description if len(w)  >= 2 ] for description in tokenised_description] \n",
    "print(tokenised_description[485])\n",
    "print(len(tokenised_description[485]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following removes stopwords from the list of tokens based on a stopwords file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after']\n"
     ]
    }
   ],
   "source": [
    "#read stop words file and add to a list\n",
    "stopwords = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "    f.close()\n",
    "print(stopwords[0:10]) # print first 10 stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "# for each description remove the word if it is in the stopword list\n",
    "tokenised_description = [[w for w in description if w not in stopwords] for description in tokenised_description]\n",
    "print(len(tokenised_description[485]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below removes words that only appear once based on term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4186\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tokenised_description)) #flatten the list of tokenised description\n",
    "term_freq = FreqDist(words) #get frequencies of words\n",
    "less_freq = set(term_freq.hapaxes()) #get the set of words that appear only once.\n",
    "print(len(less_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "#for each description remove words that appear only once in document collection\n",
    "tokenised_description = [[w for w in description if w not in less_freq] for description in tokenised_description]\n",
    "print(len(tokenised_description[485]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below removes the top 50 most frequent words based on document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'experience', 'role', 'work', 'team', 'working', 'skills', 'client', 'job', 'company', 'business', 'uk', 'excellent', 'management', 'based', 'apply', 'opportunity', 'salary', 'required', 'successful', 'support', 'join', 'candidate', 'service', 'knowledge', 'development', 'leading', 'high', 'cv', 'manager', 'www', 'training', 'sales', 'strong', 'provide', 'including', 'services', 'ability', 'contact', 'position', 'recruitment', 'full', 'benefits', 'posted', 'jobseeking', 'originally', 'include', 'clients', 'good', 'essential']\n"
     ]
    }
   ],
   "source": [
    "#get the 50 most frequent words based on document frequency\n",
    "words_2 = list(chain.from_iterable([set(description) \\\n",
    "                                    for description in tokenised_description])) # get set of unique words for that article\n",
    "doc_freq = FreqDist(words_2)\n",
    "most_freq_doc = []\n",
    "freq_doc = doc_freq.most_common(50) #output : list of tuple (word,freq)\n",
    "#append 50 most common words to a list\n",
    "for i in freq_doc:\n",
    "    most_freq_doc.append(i[0])\n",
    "print(most_freq_doc)#check the list if they contain the words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "#for each description remove top 50 most common words based on document freq\n",
    "tokenised_description = [[w for w in description if w not in most_freq_doc] for description in tokenised_description]\n",
    "print(len(tokenised_description[485]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"uk's\", 'care', 'providers', 'recruit', 'assisted', 'living', 'coordinator', 'facility', 'westbourne', 'assisted', 'living', 'coordinator', 'responsibility', 'day', 'day', 'assisted', 'living', 'functions', 'assisted', 'living', 'coordinator', 'reports', 'registered', 'matters', 'relating', 'regulatory', 'standards', 'set', 'cqc', 'responsibilities', 'resident', 'programmes', 'assurance', 'regulatory', 'people', 'facility', 'registered', 'nursing', 'rgn', 'demonstrates', 'career', 'path', 'elderly', 'care', 'westbourne', 'area', 'asap', 'early', 'interview']\n"
     ]
    }
   ],
   "source": [
    "print(tokenised_description[485])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Now that we have finish the pre-processing steps we shall now save the vocab as specified in the assignment specification as it will be required in task 2 and 3.\n",
    "\n",
    "Additionally, we shall also save:\n",
    "- list of tokens for each job description\n",
    "- web index\n",
    "- class of each job \n",
    "- title of each job\n",
    "\n",
    "as these will be also be used in task 2 and 3, but are not required in the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save pre-processed data for task 2 and 3\n",
    "def save_tokens(filename,description_tokens):\n",
    "    token_file = open(filename, 'w')\n",
    "    string = '\\n'. join([', '.join(token) for token in description_tokens]) #for each description append tokens with ','before /n at the end\n",
    "    token_file.write(string)\n",
    "    token_file.close()\n",
    "def save_webindex(filename,webindex):\n",
    "    webindex_file = open(filename, 'w')\n",
    "    #save the webindex on each line\n",
    "    for i in webindex:\n",
    "        string = str(i)\n",
    "        webindex_file.write(string)\n",
    "    webindex_file.close\n",
    "def save_class(filename,target_class):\n",
    "    class_file = open(filename,'w')\n",
    "    #save the target class on each line\n",
    "    for i in target_class:\n",
    "        string = str(i) + '\\n'\n",
    "        class_file.write(string)\n",
    "    class_file.close()\n",
    "def save_title(filename, title):\n",
    "    title_file = open(filename,'w',encoding=\"utf-8\") # need encoding for char such as ****\n",
    "    for i in title:\n",
    "        string = str(i) + '\\n' \n",
    "        title_file.write(string) # write each title to a new line\n",
    "    title_file.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances (tokens):  776\n",
      "Number of instances: (job index):  776\n",
      "Number of instances: (class):  776\n",
      "Number of instances (title):  776\n"
     ]
    }
   ],
   "source": [
    "save_tokens('description_tokens.txt',tokenised_description)\n",
    "save_webindex('webindex.txt',job_index)\n",
    "save_class('class.txt',target_class)\n",
    "save_title('title.txt',title)\n",
    "\n",
    "print('Number of instances (tokens): ', len(tokenised_description))\n",
    "print('Number of instances: (job index): ', len(job_index))\n",
    "print('Number of instances: (class): ', len(target_class))\n",
    "print('Number of instances (title): ',len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5168\n"
     ]
    }
   ],
   "source": [
    "#save vocab to vocab.txt\n",
    "words_final = list(chain.from_iterable(tokenised_description)) #flatten the tokenised words of descriptions\n",
    "vocab = sorted(set(words_final)) # get all unique and save as vocab in alphabetical order\n",
    "print(len(vocab))\n",
    "vocab_file = open('vocab.txt','w')\n",
    "for ind in range(len(vocab)): # for each word in vocab list print index s\n",
    "     vocab_file.write(\"{}:{}\\n\".format(vocab[ind],ind))\n",
    "vocab_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In summary, in this task we have we extract the description portion of each file, were it underwent basic text pre-processing. This gave us the vocabulary for this task which we saved. Additionally, we also saved the tokens of the pre-processing steps as well as web index, title and class of each text file. As all of these saved output files will be used in both task 2 and 3.\n",
    "\n",
    "***NOTE ENSURE THAT ALL OUTPUTS ARE SAVED INTO THE SAME DIRECTORY AS IT WILL BE CALLED IN TASK 2 AND 3 AND WILL ONLY WORK IF THEY ARE INS THE SAME DIRECTORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
