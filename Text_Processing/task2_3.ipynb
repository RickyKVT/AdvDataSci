{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Ricky Truong\n",
    "#### Student ID: s3783560\n",
    "\n",
    "Date: XXXX\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "This notebook is comprised of 2 tasks. In task 2 we are to generate 3 feature representation for the job description of each file. The first feature is to be a count vector representation feature, where it is based on a bag of word model. The second and third feature representation are a unweighted and weighted vector representation that is to be based on word embeddings using any embedding language model.\n",
    "\n",
    "Task 3 conists of 2 different sub-parts. The first sub-parts requires us to build a 2 different classification model whose features are to be based on any of the three feature representation generated in task 2. These models are then to be compared. The second part of task 3 required us to compare if different features of the job text file will provide better results for a classification model. The first model of this task is to be built only on the title of job, the second, just the job description which we have done previously. And the final model is to be based on the title and job description of the job text file. Simiarly, these 3 models will be compared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricky\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from nltk.probability import *\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.1 will build a feature representation based on the bags of words model which 2.2 will build feature represesntation based on word embeddings of a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of words Language Model\n",
    "\n",
    "The bag of words model means that the order of words do not matter. We based our feature based on the number of words or its count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block below is just to print and confirm if we have successfully built a feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the feature representation of the first article\n",
    "def validator(data_features, vocab, tokenised_articles):\n",
    "        for word, value in zip(vocab, data_features.toarray()[0]):\n",
    "            if value > 0:\n",
    "                print(f\"{word}:{value}\", end=' ')\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we shall read in the description token which we obtained from task 1. Ensure file is in same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "['market', ' retail', ' rapid', ' growth', ' due', ' expansion', ' add', ' financial', ' planning', ' analyst', ' central', ' central', ' london', ' fantastic', ' newly', ' created', ' driving', ' forward', ' financial', ' planning', ' analysis', ' group', ' reporting', ' directly', ' head', ' fp', ' assist', ' revenue', ' analysis', ' product', ' channel', ' region', ' commercial', ' input', ' review', ' cases', ' presenting', ' proposals', ' approval', ' develop', ' endtoend', ' planning', ' cycle', ' processes', ' lead', ' regional', ' planning', ' processes', ' ensure', ' completeness', ' key', ' channels', ' products', ' finance', ' year', ' strategic', ' plan', ' addition', ' globally', ' regions', ' develop', ' capital', ' investment', ' plan', ' forward', ' thinking', ' confident', ' qualified', ' analyst', ' aca', ' cima', ' interpersonal', ' interest', ' feel', ' relevant', ' call', ' candidates', ' straight', ' big', ' audit', ' firms', ' interested', ' hays', ' specialist', ' limited', ' acts', ' employment', ' agency', ' permanent', ' employment', ' supply', ' temporary', ' workers', ' applying', ' accept', ' privacy', ' policy', ' disclaimers', ' found', ' hays']\n"
     ]
    }
   ],
   "source": [
    "#read in job description file and tokenise each line\n",
    "#one line = one job descrption\n",
    "description_tokens = []\n",
    "with open('description_tokens.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        tokens = list(line.strip().split(',')) #for each line strip the '\\n' and split based on ,\n",
    "        description_tokens.append(tokens) # append tokens list to list of tokens\n",
    "    f.close()\n",
    "# print len and first list to confirm\n",
    "print(len(description_tokens))\n",
    "print(description_tokens[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also read in the vocab file which we also obtained and saved in task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5168\n"
     ]
    }
   ],
   "source": [
    "# read in vocab and save as a dictionary format\n",
    "#where key:value = word:word index\n",
    "dict_vocab = {}\n",
    "with open('vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        vocab = line.strip().split(':') # split line based of ':'\n",
    "        dict_vocab[vocab[0]] = vocab[1] #assign key as first element (word) and value as second element (index)\n",
    "    f.close()\n",
    "print(len(dict_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to build the count feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776, 5168)\n"
     ]
    }
   ],
   "source": [
    "vocab = list(dict_vocab.keys()) # converts all keys (words) to a list of words = vocab\n",
    "cVectoriser =CountVectorizer(analyzer= 'word', vocabulary=vocab)\n",
    "\n",
    "#creates a string of tokens(job description, and for each string fit to count_vectorizer\n",
    "count_features = cVectoriser.fit_transform([\"\".join(description) for description in description_tokens])\n",
    "print(count_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aca:1 accept:1 acts:1 add:1 addition:1 agency:1 analysis:2 analyst:2 applying:1 approval:1 assist:1 audit:1 big:1 call:1 candidates:1 capital:1 cases:1 central:2 channel:1 channels:1 cima:1 commercial:1 completeness:1 confident:1 created:1 cycle:1 develop:2 directly:1 disclaimers:1 driving:1 due:1 employment:2 endtoend:1 ensure:1 expansion:1 fantastic:1 feel:1 finance:1 financial:2 firms:1 forward:2 found:1 fp:1 globally:1 group:1 growth:1 hays:2 head:1 input:1 interest:1 interested:1 interpersonal:1 investment:1 key:1 lead:1 limited:1 london:1 market:1 newly:1 permanent:1 plan:2 planning:4 policy:1 presenting:1 privacy:1 processes:2 product:1 products:1 proposals:1 qualified:1 rapid:1 region:1 regional:1 regions:1 relevant:1 reporting:1 retail:1 revenue:1 review:1 specialist:1 straight:1 strategic:1 supply:1 temporary:1 thinking:1 workers:1 year:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check if it works by printing first article\n",
    "validator(count_features, vocab, description_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Word Embeddings Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unweighted Vector\n",
    "For this section we are using a pre train language model (word2vec), that has been trained on google-news-300. We shall use this model to create word embeddings in our document based on the tokens of descrptions that we have provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below generates our web embeddings where it takes our pre-train word embeddings and tokens to produce vector representation based on our tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unweighted word embeddings\n",
    "#generate vector representation of document\n",
    "def docvecs(embeddings, tokens):\n",
    "    vecs = np.zeros((len(tokens), embeddings.vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        valid_keys = [term for term in token if term in embeddings.key_to_index] # append words that appear in pretrain and tokens\n",
    "        if valid_keys: #check for valid key b/c sometimes valid keys is empty and will result in error\n",
    "            docvec = np.vstack([embeddings[term] for term in valid_keys]) # create vector\n",
    "            docvec = np.sum(docvec, axis=0) \n",
    "            vecs[i,:] = docvec\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file is 1.6gb so it might take a while to run\n",
    "preTW2v_wv = api.load('word2vec-google-news-300') # load the pre train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe for the tokens as the function to above needs to iterate both the index and tokens of that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [market,  retail,  rapid,  growth,  due,  expa...\n",
       "1      [responsible,  efficient,  running,  accountin...\n",
       "2      [purpose,  advice,  telephone,  leads,  suppli...\n",
       "3      [expanding,  recruit,  aca,  qualified,  accou...\n",
       "4      [offers,  interesting,  part,  qualified,  fin...\n",
       "                             ...                        \n",
       "771    [arisen,  property,  valuer,  liverpool,  area...\n",
       "772    [yorkshire,  established,  telecoms,  data,  c...\n",
       "773    [location,  peterborough,  title,  marketing, ...\n",
       "774    [outskirts,  poole,  dorset,  recruiting,  int...\n",
       "775    [senior,  negotiator,  location,  shoreditch, ...\n",
       "Name: tokens, Length: 776, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creation of dataframe for tokens\n",
    "df = pd.DataFrame()\n",
    "df['tokens'] = description_tokens\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unweighted vectors:  232800\n"
     ]
    }
   ],
   "source": [
    "preTW2v_dvs = docvecs(preTW2v_wv,df['tokens']) # generate document vector representation\n",
    "print('Total number of unweighted vectors: ',preTW2v_dvs.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted Vector\n",
    "For this section we will build a weight vector that is TF-IDF weighted word vector. We shall we using the same pre-train model provided in the previous section as well as the list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first have to create a mapping between a word in the vocab as its respective weight. To do this we first must get the vocabulary where it maps each word to an index as well as the td-idf vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aap': '0',\n",
       " 'aaron': '1',\n",
       " 'aat': '2',\n",
       " 'abb': '3',\n",
       " 'abenefit': '4',\n",
       " 'aberdeen': '5',\n",
       " 'abi': '6',\n",
       " 'abilities': '7',\n",
       " 'abreast': '8',\n",
       " 'abroad': '9',\n",
       " 'absence': '10',\n",
       " 'absolute': '11',\n",
       " 'ac': '12',\n",
       " 'aca': '13',\n",
       " 'academic': '14',\n",
       " 'academy': '15',\n",
       " 'acca': '16',\n",
       " 'accept': '17',\n",
       " 'acceptable': '18',\n",
       " 'acceptance': '19',\n",
       " 'accepted': '20',\n",
       " 'access': '21',\n",
       " 'accessible': '22',\n",
       " 'accident': '23',\n",
       " 'accommodates': '24',\n",
       " 'accommodation': '25',\n",
       " 'accomplished': '26',\n",
       " 'accordance': '27',\n",
       " 'account': '28',\n",
       " 'accountabilities': '29',\n",
       " 'accountability': '30',\n",
       " 'accountable': '31',\n",
       " 'accountancy': '32',\n",
       " 'accountant': '33',\n",
       " 'accountants': '34',\n",
       " 'accounting': '35',\n",
       " 'accounts': '36',\n",
       " 'accreditation': '37',\n",
       " 'accredited': '38',\n",
       " 'accruals': '39',\n",
       " 'accuracy': '40',\n",
       " 'accurate': '41',\n",
       " 'accurately': '42',\n",
       " 'achievable': '43',\n",
       " 'achieve': '44',\n",
       " 'achieved': '45',\n",
       " 'achievement': '46',\n",
       " 'achievements': '47',\n",
       " 'achiever': '48',\n",
       " 'achieving': '49',\n",
       " 'acii': '50',\n",
       " 'acquired': '51',\n",
       " 'acquisition': '52',\n",
       " 'acquisitions': '53',\n",
       " 'act': '54',\n",
       " 'acting': '55',\n",
       " 'action': '56',\n",
       " 'actions': '57',\n",
       " 'actionscript': '58',\n",
       " 'active': '59',\n",
       " 'actively': '60',\n",
       " 'activites': '61',\n",
       " 'activities': '62',\n",
       " 'activity': '63',\n",
       " 'acts': '64',\n",
       " 'actual': '65',\n",
       " 'actuarial': '66',\n",
       " 'acumen': '67',\n",
       " 'acute': '68',\n",
       " 'ad': '69',\n",
       " 'adam': '70',\n",
       " 'adapt': '71',\n",
       " 'adaptability': '72',\n",
       " 'add': '73',\n",
       " 'added': '74',\n",
       " 'addiction': '75',\n",
       " 'adding': '76',\n",
       " 'addition': '77',\n",
       " 'additional': '78',\n",
       " 'additionally': '79',\n",
       " 'additions': '80',\n",
       " 'address': '81',\n",
       " 'addresses': '82',\n",
       " 'addressing': '83',\n",
       " 'adecco': '84',\n",
       " 'adept': '85',\n",
       " 'adequacy': '86',\n",
       " 'adequate': '87',\n",
       " 'adequately': '88',\n",
       " 'adhere': '89',\n",
       " 'adhered': '90',\n",
       " 'adherence': '91',\n",
       " 'adhering': '92',\n",
       " 'adhoc': '93',\n",
       " 'adjust': '94',\n",
       " 'adjuster': '95',\n",
       " 'adjusting': '96',\n",
       " 'adjustments': '97',\n",
       " 'admin': '98',\n",
       " 'administer': '99',\n",
       " 'administered': '100',\n",
       " 'administering': '101',\n",
       " 'administration': '102',\n",
       " 'administrative': '103',\n",
       " 'administrator': '104',\n",
       " 'administrators': '105',\n",
       " 'admission': '106',\n",
       " 'admissions': '107',\n",
       " 'adobe': '108',\n",
       " 'adolescents': '109',\n",
       " 'adopt': '110',\n",
       " 'adopted': '111',\n",
       " 'adopting': '112',\n",
       " 'adrian': '113',\n",
       " 'ads': '114',\n",
       " 'adstream': '115',\n",
       " 'adult': '116',\n",
       " 'adults': '117',\n",
       " 'advance': '118',\n",
       " 'advanced': '119',\n",
       " 'advancement': '120',\n",
       " 'advances': '121',\n",
       " 'advantage': '122',\n",
       " 'advantageous': '123',\n",
       " 'advert': '124',\n",
       " 'advertise': '125',\n",
       " 'advertised': '126',\n",
       " 'advertisement': '127',\n",
       " 'advertising': '128',\n",
       " 'advertsing': '129',\n",
       " 'advice': '130',\n",
       " 'advise': '131',\n",
       " 'advised': '132',\n",
       " 'adviser': '133',\n",
       " 'advisers': '134',\n",
       " 'advising': '135',\n",
       " 'advisor': '136',\n",
       " 'advisors': '137',\n",
       " 'advisory': '138',\n",
       " 'aeronautical': '139',\n",
       " 'aerospace': '140',\n",
       " 'affairs': '141',\n",
       " 'affect': '142',\n",
       " 'affected': '143',\n",
       " 'affecting': '144',\n",
       " 'affinity': '145',\n",
       " 'affordable': '146',\n",
       " 'africa': '147',\n",
       " 'aftermarket': '148',\n",
       " 'afternoon': '149',\n",
       " 'afternoons': '150',\n",
       " 'age': '151',\n",
       " 'aged': '152',\n",
       " 'agencies': '153',\n",
       " 'agency': '154',\n",
       " 'agent': '155',\n",
       " 'agents': '156',\n",
       " 'ages': '157',\n",
       " 'aggressive': '158',\n",
       " 'agile': '159',\n",
       " 'ago': '160',\n",
       " 'agree': '161',\n",
       " 'agreed': '162',\n",
       " 'agreement': '163',\n",
       " 'agreements': '164',\n",
       " 'agy': '165',\n",
       " 'ahead': '166',\n",
       " 'ahu': '167',\n",
       " 'aid': '168',\n",
       " 'aided': '169',\n",
       " 'aids': '170',\n",
       " 'aim': '171',\n",
       " 'aimed': '172',\n",
       " 'aims': '173',\n",
       " 'air': '174',\n",
       " 'aircraft': '175',\n",
       " 'airdrie': '176',\n",
       " 'alarm': '177',\n",
       " 'alarms': '178',\n",
       " 'albans': '179',\n",
       " 'alcohol': '180',\n",
       " 'aldredirweb': '181',\n",
       " 'alecto': '182',\n",
       " 'alerting': '183',\n",
       " 'alex': '184',\n",
       " 'algorithm': '185',\n",
       " 'algorithms': '186',\n",
       " 'aligned': '187',\n",
       " 'alignment': '188',\n",
       " 'alike': '189',\n",
       " 'allied': '190',\n",
       " 'allocated': '191',\n",
       " 'allocation': '192',\n",
       " 'allowance': '193',\n",
       " 'allowances': '194',\n",
       " 'allowing': '195',\n",
       " 'alm': '196',\n",
       " 'alongside': '197',\n",
       " 'alternate': '198',\n",
       " 'alternative': '199',\n",
       " 'alternatively': '200',\n",
       " 'altrincham': '201',\n",
       " 'aluminium': '202',\n",
       " 'amazing': '203',\n",
       " 'ambassador': '204',\n",
       " 'ambition': '205',\n",
       " 'ambitions': '206',\n",
       " 'ambitious': '207',\n",
       " 'ambridge': '208',\n",
       " 'amenities': '209',\n",
       " 'america': '210',\n",
       " 'american': '211',\n",
       " 'amigo': '212',\n",
       " 'aml': '213',\n",
       " 'amount': '214',\n",
       " 'amounts': '215',\n",
       " 'amp': '216',\n",
       " 'amrywiol': '217',\n",
       " 'anaesthetics': '218',\n",
       " 'anaesthetist': '219',\n",
       " 'analog': '220',\n",
       " 'analogue': '221',\n",
       " 'analyse': '222',\n",
       " 'analysers': '223',\n",
       " 'analyses': '224',\n",
       " 'analysing': '225',\n",
       " 'analysis': '226',\n",
       " 'analyst': '227',\n",
       " 'analystbelfast': '228',\n",
       " 'analysts': '229',\n",
       " 'analytical': '230',\n",
       " 'analytics': '231',\n",
       " 'ancillary': '232',\n",
       " 'andoffers': '233',\n",
       " 'andrew': '234',\n",
       " 'android': '235',\n",
       " 'androidoptometryapp': '236',\n",
       " 'andy': '237',\n",
       " 'angiography': '238',\n",
       " 'anglia': '239',\n",
       " 'annual': '240',\n",
       " 'annum': '241',\n",
       " 'anomalies': '242',\n",
       " 'answer': '243',\n",
       " 'answered': '244',\n",
       " 'answering': '245',\n",
       " 'anthony': '246',\n",
       " 'anticipate': '247',\n",
       " 'anticipated': '248',\n",
       " 'antony': '249',\n",
       " 'apartments': '250',\n",
       " 'api': '251',\n",
       " 'apmp': '252',\n",
       " 'appearance': '253',\n",
       " 'appetite': '254',\n",
       " 'appleoptometry': '255',\n",
       " 'applicable': '256',\n",
       " 'applicant': '257',\n",
       " 'applicants': '258',\n",
       " 'application': '259',\n",
       " 'applications': '260',\n",
       " 'applied': '261',\n",
       " 'applies': '262',\n",
       " 'applying': '263',\n",
       " 'appoint': '264',\n",
       " 'appointed': '265',\n",
       " 'appointment': '266',\n",
       " 'appointments': '267',\n",
       " 'appraisal': '268',\n",
       " 'appraisals': '269',\n",
       " 'appraising': '270',\n",
       " 'appreciation': '271',\n",
       " 'apprentice': '272',\n",
       " 'apprenticeship': '273',\n",
       " 'approach': '274',\n",
       " 'approachable': '275',\n",
       " 'approaches': '276',\n",
       " 'appropriately': '277',\n",
       " 'approval': '278',\n",
       " 'approved': '279',\n",
       " 'approx': '280',\n",
       " 'approximately': '281',\n",
       " 'aptitude': '282',\n",
       " 'aptrack': '283',\n",
       " 'ar': '284',\n",
       " 'architect': '285',\n",
       " 'architects': '286',\n",
       " 'architectural': '287',\n",
       " 'architecture': '288',\n",
       " 'architectures': '289',\n",
       " 'archiving': '290',\n",
       " 'area': '291',\n",
       " 'areas': '292',\n",
       " 'arena': '293',\n",
       " 'arise': '294',\n",
       " 'arisen': '295',\n",
       " 'arm': '296',\n",
       " 'armed': '297',\n",
       " 'arrange': '298',\n",
       " 'arranged': '299',\n",
       " 'arrangement': '300',\n",
       " 'arrangements': '301',\n",
       " 'arranging': '302',\n",
       " 'array': '303',\n",
       " 'arrow': '304',\n",
       " 'art': '305',\n",
       " 'articulate': '306',\n",
       " 'articulating': '307',\n",
       " 'artists': '308',\n",
       " 'asap': '309',\n",
       " 'asbestos': '310',\n",
       " 'ascertain': '311',\n",
       " 'ascot': '312',\n",
       " 'asda': '313',\n",
       " 'ashby': '314',\n",
       " 'ashford': '315',\n",
       " 'ashley': '316',\n",
       " 'asia': '317',\n",
       " 'asic': '318',\n",
       " 'asked': '319',\n",
       " 'asp': '320',\n",
       " 'aspect': '321',\n",
       " 'aspects': '322',\n",
       " 'aspirations': '323',\n",
       " 'aspire': '324',\n",
       " 'aspireinc': '325',\n",
       " 'assembler': '326',\n",
       " 'assemblies': '327',\n",
       " 'assembly': '328',\n",
       " 'assertive': '329',\n",
       " 'assess': '330',\n",
       " 'assessed': '331',\n",
       " 'assessing': '332',\n",
       " 'assessment': '333',\n",
       " 'assessments': '334',\n",
       " 'assessor': '335',\n",
       " 'assessors': '336',\n",
       " 'asset': '337',\n",
       " 'assets': '338',\n",
       " 'assigned': '339',\n",
       " 'assignment': '340',\n",
       " 'assignments': '341',\n",
       " 'assimilate': '342',\n",
       " 'assist': '343',\n",
       " 'assistance': '344',\n",
       " 'assistant': '345',\n",
       " 'assistants': '346',\n",
       " 'assisted': '347',\n",
       " 'assisting': '348',\n",
       " 'assists': '349',\n",
       " 'associate': '350',\n",
       " 'associates': '351',\n",
       " 'association': '352',\n",
       " 'assume': '353',\n",
       " 'assuming': '354',\n",
       " 'assumptions': '355',\n",
       " 'assurance': '356',\n",
       " 'assured': '357',\n",
       " 'astute': '358',\n",
       " 'ate': '359',\n",
       " 'atmosphere': '360',\n",
       " 'att': '361',\n",
       " 'attached': '362',\n",
       " 'attend': '363',\n",
       " 'attendance': '364',\n",
       " 'attending': '365',\n",
       " 'attention': '366',\n",
       " 'attitude': '367',\n",
       " 'attitudes': '368',\n",
       " 'attract': '369',\n",
       " 'attracting': '370',\n",
       " 'attractive': '371',\n",
       " 'attracts': '372',\n",
       " 'attributes': '373',\n",
       " 'audi': '374',\n",
       " 'audience': '375',\n",
       " 'audiences': '376',\n",
       " 'audio': '377',\n",
       " 'audit': '378',\n",
       " 'auditing': '379',\n",
       " 'auditor': '380',\n",
       " 'auditors': '381',\n",
       " 'audits': '382',\n",
       " 'authorisation': '383',\n",
       " 'authorised': '384',\n",
       " 'authorities': '385',\n",
       " 'authority': '386',\n",
       " 'authorized': '387',\n",
       " 'autism': '388',\n",
       " 'auto': '389',\n",
       " 'autocad': '390',\n",
       " 'autodesk': '391',\n",
       " 'automated': '392',\n",
       " 'automatic': '393',\n",
       " 'automatically': '394',\n",
       " 'automation': '395',\n",
       " 'automotive': '396',\n",
       " 'autonomous': '397',\n",
       " 'autonomously': '398',\n",
       " 'autonomy': '399',\n",
       " 'auxiliaries': '400',\n",
       " 'auxiliary': '401',\n",
       " 'av': '402',\n",
       " 'availability': '403',\n",
       " 'avenues': '404',\n",
       " 'average': '405',\n",
       " 'avon': '406',\n",
       " 'avse': '407',\n",
       " 'award': '408',\n",
       " 'awarded': '409',\n",
       " 'awards': '410',\n",
       " 'awardwinning': '411',\n",
       " 'aware': '412',\n",
       " 'awareness': '413',\n",
       " 'ax': '414',\n",
       " 'axapta': '415',\n",
       " 'axis': '416',\n",
       " 'aylesbury': '417',\n",
       " 'ba': '418',\n",
       " 'bachelor': '419',\n",
       " \"bachelor's\": '420',\n",
       " 'back': '421',\n",
       " 'backed': '422',\n",
       " 'backend': '423',\n",
       " 'background': '424',\n",
       " 'backgrounds': '425',\n",
       " 'backing': '426',\n",
       " 'backtesting': '427',\n",
       " 'backup': '428',\n",
       " 'bacs': '429',\n",
       " 'badenoch': '430',\n",
       " 'bailey': '431',\n",
       " 'baking': '432',\n",
       " 'balance': '433',\n",
       " 'balances': '434',\n",
       " 'band': '435',\n",
       " 'bank': '436',\n",
       " 'banking': '437',\n",
       " 'banks': '438',\n",
       " 'barchester': '439',\n",
       " \"barchester's\": '440',\n",
       " 'barns': '441',\n",
       " 'barnsley': '442',\n",
       " 'barry': '443',\n",
       " 'base': '444',\n",
       " 'baseband': '445',\n",
       " 'basel': '446',\n",
       " 'bases': '447',\n",
       " 'basic': '448',\n",
       " 'basingstoke': '449',\n",
       " 'basis': '450',\n",
       " 'batch': '451',\n",
       " 'bath': '452',\n",
       " 'bathing': '453',\n",
       " 'bathroom': '454',\n",
       " 'bathrooms': '455',\n",
       " 'bdm': '456',\n",
       " 'beacon': '457',\n",
       " 'bearings': '458',\n",
       " 'beautiful': '459',\n",
       " 'becky': '460',\n",
       " 'bed': '461',\n",
       " 'bedded': '462',\n",
       " 'bedford': '463',\n",
       " 'bedfordshire': '464',\n",
       " 'beds': '465',\n",
       " 'began': '466',\n",
       " 'begin': '467',\n",
       " 'behalf': '468',\n",
       " 'behaviour': '469',\n",
       " 'behavioural': '470',\n",
       " 'behaviours': '471',\n",
       " 'belfast': '472',\n",
       " 'belief': '473',\n",
       " 'believes': '474',\n",
       " 'bell': '475',\n",
       " 'belt': '476',\n",
       " 'ben': '477',\n",
       " 'bench': '478',\n",
       " 'beneficial': '479',\n",
       " 'beneficiaries': '480',\n",
       " 'benefit': '481',\n",
       " 'benelux': '482',\n",
       " 'beng': '483',\n",
       " 'benn': '484',\n",
       " 'bens': '485',\n",
       " 'berkshire': '486',\n",
       " 'bespoke': '487',\n",
       " 'beverage': '488',\n",
       " 'bexhill': '489',\n",
       " 'bi': '490',\n",
       " 'bias': '491',\n",
       " 'biased': '492',\n",
       " 'bid': '493',\n",
       " 'bids': '494',\n",
       " 'big': '495',\n",
       " 'biggest': '496',\n",
       " 'bikes': '497',\n",
       " 'billing': '498',\n",
       " 'billinghay': '499',\n",
       " 'billion': '500',\n",
       " 'bills': '501',\n",
       " 'biology': '502',\n",
       " 'birmingham': '503',\n",
       " 'birthdays': '504',\n",
       " 'bit': '505',\n",
       " 'black': '506',\n",
       " 'blackburn': '507',\n",
       " 'blake': '508',\n",
       " 'blind': '509',\n",
       " 'blocked': '510',\n",
       " 'blood': '511',\n",
       " 'bls': '512',\n",
       " 'blue': '513',\n",
       " 'bluebird': '514',\n",
       " 'bluechip': '515',\n",
       " 'bluestones': '516',\n",
       " 'bms': '517',\n",
       " 'bmsbmsit': '518',\n",
       " 'bmsgraduates': '519',\n",
       " 'bmsit': '520',\n",
       " 'bmsuk': '521',\n",
       " 'bn': '522',\n",
       " 'board': '523',\n",
       " 'boast': '524',\n",
       " 'boasts': '525',\n",
       " 'bodies': '526',\n",
       " 'body': '527',\n",
       " 'boiler': '528',\n",
       " 'boilers': '529',\n",
       " 'bolting': '530',\n",
       " 'bonus': '531',\n",
       " 'bonuses': '532',\n",
       " 'book': '533',\n",
       " 'booked': '534',\n",
       " 'booking': '535',\n",
       " 'bookings': '536',\n",
       " 'bookkeeper': '537',\n",
       " 'bookkeeping': '538',\n",
       " 'books': '539',\n",
       " 'boost': '540',\n",
       " 'border': '541',\n",
       " 'boring': '542',\n",
       " 'boss': '543',\n",
       " 'boundaries': '544',\n",
       " 'bournemouth': '545',\n",
       " 'boutique': '546',\n",
       " 'box': '547',\n",
       " 'boxes': '548',\n",
       " 'brace': '549',\n",
       " 'bracknell': '550',\n",
       " 'bradford': '551',\n",
       " 'brain': '552',\n",
       " 'branch': '553',\n",
       " 'branches': '554',\n",
       " 'brand': '555',\n",
       " 'branded': '556',\n",
       " 'brands': '557',\n",
       " 'brbrjob': '558',\n",
       " 'break': '559',\n",
       " 'breakdown': '560',\n",
       " 'breakdowns': '561',\n",
       " 'breaking': '562',\n",
       " 'breheny': '563',\n",
       " 'brian': '564',\n",
       " 'briant': '565',\n",
       " 'bridge': '566',\n",
       " 'briefing': '567',\n",
       " 'briefings': '568',\n",
       " 'briefs': '569',\n",
       " 'bright': '570',\n",
       " 'brighton': '571',\n",
       " 'brilliant': '572',\n",
       " 'bring': '573',\n",
       " 'bringing': '574',\n",
       " 'brings': '575',\n",
       " 'bristol': '576',\n",
       " 'britain': '577',\n",
       " \"britain's\": '578',\n",
       " 'british': '579',\n",
       " 'brixton': '580',\n",
       " 'broad': '581',\n",
       " 'broaden': '582',\n",
       " 'broken': '583',\n",
       " 'broker': '584',\n",
       " 'brokerage': '585',\n",
       " 'brokers': '586',\n",
       " 'broking': '587',\n",
       " 'bromley': '588',\n",
       " 'broughton': '589',\n",
       " 'brown': '590',\n",
       " 'brsalary': '591',\n",
       " 'bs': '592',\n",
       " 'bsc': '593',\n",
       " 'bubble': '594',\n",
       " 'buckinghamshire': '595',\n",
       " 'buckle': '596',\n",
       " 'budget': '597',\n",
       " 'budgetary': '598',\n",
       " 'budgeted': '599',\n",
       " 'budgeting': '600',\n",
       " 'budgets': '601',\n",
       " 'bug': '602',\n",
       " 'build': '603',\n",
       " 'building': '604',\n",
       " 'buildings': '605',\n",
       " 'builds': '606',\n",
       " 'built': '607',\n",
       " 'bulk': '608',\n",
       " 'bull': '609',\n",
       " 'bullability': '610',\n",
       " 'bupa': '611',\n",
       " 'bureau': '612',\n",
       " 'bureaux': '613',\n",
       " 'burgess': '614',\n",
       " 'burscough': '615',\n",
       " 'bury': '616',\n",
       " 'bus': '617',\n",
       " 'businesses': '618',\n",
       " 'busy': '619',\n",
       " 'butler': '620',\n",
       " 'button': '621',\n",
       " 'buy': '622',\n",
       " 'buyer': '623',\n",
       " 'buyers': '624',\n",
       " 'buying': '625',\n",
       " 'buyside': '626',\n",
       " 'buytolet': '627',\n",
       " 'bydwragedd': '628',\n",
       " 'bydwreigiaeth': '629',\n",
       " 'ca': '630',\n",
       " 'cable': '631',\n",
       " 'cabling': '632',\n",
       " 'cad': '633',\n",
       " 'cadstar': '634',\n",
       " 'cae': '635',\n",
       " 'calco': '636',\n",
       " 'calculation': '637',\n",
       " 'calculations': '638',\n",
       " 'calculus': '639',\n",
       " 'calibrating': '640',\n",
       " 'calibration': '641',\n",
       " 'calibre': '642',\n",
       " 'california': '643',\n",
       " 'call': '644',\n",
       " 'called': '645',\n",
       " 'caller': '646',\n",
       " 'calling': '647',\n",
       " 'calls': '648',\n",
       " 'calm': '649',\n",
       " 'cam': '650',\n",
       " 'cambridge': '651',\n",
       " 'cambridgeshire': '652',\n",
       " 'cameras': '653',\n",
       " 'camhs': '654',\n",
       " 'campaign': '655',\n",
       " 'campaigns': '656',\n",
       " 'canada': '657',\n",
       " 'cancer': '658',\n",
       " 'candidates': '659',\n",
       " 'cannock': '660',\n",
       " 'canteen': '661',\n",
       " 'canvassing': '662',\n",
       " 'capabilities': '663',\n",
       " 'capability': '664',\n",
       " 'capable': '665',\n",
       " 'capacity': '666',\n",
       " 'capex': '667',\n",
       " 'capita': '668',\n",
       " 'capital': '669',\n",
       " 'capture': '670',\n",
       " 'car': '671',\n",
       " 'carbon': '672',\n",
       " 'carcraft': '673',\n",
       " 'card': '674',\n",
       " 'cardiac': '675',\n",
       " 'cardiff': '676',\n",
       " 'cardiology': '677',\n",
       " 'cards': '678',\n",
       " 'care': '679',\n",
       " 'cared': '680',\n",
       " 'career': '681',\n",
       " 'careers': '682',\n",
       " 'carefully': '683',\n",
       " 'carer': '684',\n",
       " 'carers': '685',\n",
       " 'cares': '686',\n",
       " 'caribbean': '687',\n",
       " 'carillion': '688',\n",
       " 'carillon': '689',\n",
       " 'caring': '690',\n",
       " 'carl': '691',\n",
       " 'caroline': '692',\n",
       " 'carpet': '693',\n",
       " 'carpets': '694',\n",
       " 'carried': '695',\n",
       " 'carrier': '696',\n",
       " 'carriers': '697',\n",
       " 'carries': '698',\n",
       " 'carry': '699',\n",
       " 'carrying': '700',\n",
       " 'cars': '701',\n",
       " 'cart': '702',\n",
       " 'case': '703',\n",
       " 'caseload': '704',\n",
       " 'cases': '705',\n",
       " 'casework': '706',\n",
       " 'cash': '707',\n",
       " 'cashflow': '708',\n",
       " 'cashier': '709',\n",
       " 'cashiering': '710',\n",
       " 'castings': '711',\n",
       " 'casual': '712',\n",
       " 'casualty': '713',\n",
       " 'catalyst': '714',\n",
       " 'categories': '715',\n",
       " 'catering': '716',\n",
       " 'caterpillar': '717',\n",
       " 'caters': '718',\n",
       " 'catia': '719',\n",
       " 'ccab': '720',\n",
       " 'ccn': '721',\n",
       " 'cct': '722',\n",
       " 'cctv': '723',\n",
       " 'cduttoncompassltd': '724',\n",
       " 'celebrating': '725',\n",
       " 'celesio': '726',\n",
       " 'cell': '727',\n",
       " 'cells': '728',\n",
       " 'cellular': '729',\n",
       " 'cemap': '730',\n",
       " 'ceng': '731',\n",
       " 'central': '732',\n",
       " 'centrally': '733',\n",
       " 'centre': '734',\n",
       " \"centre's\": '735',\n",
       " 'centred': '736',\n",
       " 'centres': '737',\n",
       " 'century': '738',\n",
       " 'ceo': '739',\n",
       " 'cerebral': '740',\n",
       " 'certificate': '741',\n",
       " 'certificates': '742',\n",
       " 'certification': '743',\n",
       " 'certifications': '744',\n",
       " 'certified': '745',\n",
       " 'certus': '746',\n",
       " 'cfo': '747',\n",
       " 'chain': '748',\n",
       " 'chains': '749',\n",
       " 'chairing': '750',\n",
       " 'challenge': '751',\n",
       " 'challenges': '752',\n",
       " 'challenging': '753',\n",
       " 'champion': '754',\n",
       " 'chance': '755',\n",
       " 'change': '756',\n",
       " 'changed': '757',\n",
       " 'changing': '758',\n",
       " 'channel': '759',\n",
       " 'channels': '760',\n",
       " 'character': '761',\n",
       " 'characteristics': '762',\n",
       " 'charge': '763',\n",
       " 'charged': '764',\n",
       " 'charismatic': '765',\n",
       " 'charitable': '766',\n",
       " 'charities': '767',\n",
       " 'charity': '768',\n",
       " 'chartered': '769',\n",
       " 'charterhouse': '770',\n",
       " 'chartership': '771',\n",
       " 'chase': '772',\n",
       " 'chasemedical': '773',\n",
       " 'chat': '774',\n",
       " 'check': '775',\n",
       " 'checkable': '776',\n",
       " 'checking': '777',\n",
       " 'checks': '778',\n",
       " 'chelmsford': '779',\n",
       " 'cheltenham': '780',\n",
       " 'chemical': '781',\n",
       " 'cheques': '782',\n",
       " 'cheshire': '783',\n",
       " 'chester': '784',\n",
       " 'chesterfield': '785',\n",
       " 'chi': '786',\n",
       " 'chichester': '787',\n",
       " 'chief': '788',\n",
       " 'child': '789',\n",
       " \"child's\": '790',\n",
       " 'childcare': '791',\n",
       " 'children': '792',\n",
       " \"children's\": '793',\n",
       " 'chip': '794',\n",
       " 'choice': '795',\n",
       " 'choices': '796',\n",
       " 'choose': '797',\n",
       " 'chosen': '798',\n",
       " 'chris': '799',\n",
       " 'christmas': '800',\n",
       " 'church': '801',\n",
       " 'chwefror': '802',\n",
       " 'cima': '803',\n",
       " 'cinema': '804',\n",
       " 'circa': '805',\n",
       " 'circuit': '806',\n",
       " 'circuitry': '807',\n",
       " 'circuits': '808',\n",
       " 'circumstances': '809',\n",
       " 'cis': '810',\n",
       " 'cisco': '811',\n",
       " 'citizen': '812',\n",
       " 'citizens': '813',\n",
       " 'city': '814',\n",
       " 'civil': '815',\n",
       " 'civils': '816',\n",
       " 'claims': '817',\n",
       " 'clarity': '818',\n",
       " 'clark': '819',\n",
       " 'class': '820',\n",
       " 'classes': '821',\n",
       " 'classification': '822',\n",
       " 'clean': '823',\n",
       " 'cleaning': '824',\n",
       " 'cleanliness': '825',\n",
       " 'clear': '826',\n",
       " 'clearance': '827',\n",
       " 'cleared': '828',\n",
       " 'clerical': '829',\n",
       " 'clerk': '830',\n",
       " 'click': '831',\n",
       " 'clicking': '832',\n",
       " \"client's\": '833',\n",
       " 'clientfacing': '834',\n",
       " 'climate': '835',\n",
       " 'climb': '836',\n",
       " 'climbing': '837',\n",
       " 'clinic': '838',\n",
       " 'clinical': '839',\n",
       " 'clinically': '840',\n",
       " 'clinicians': '841',\n",
       " 'clinics': '842',\n",
       " 'close': '843',\n",
       " 'closed': '844',\n",
       " 'closely': '845',\n",
       " 'closer': '846',\n",
       " 'closes': '847',\n",
       " 'closing': '848',\n",
       " 'clothing': '849',\n",
       " 'cloud': '850',\n",
       " 'club': '851',\n",
       " 'clustering': '852',\n",
       " 'cmc': '853',\n",
       " 'cmm': '854',\n",
       " \"cmm's\": '855',\n",
       " 'cms': '856',\n",
       " 'cnc': '857',\n",
       " 'coach': '858',\n",
       " 'coaching': '859',\n",
       " 'code': '860',\n",
       " 'coded': '861',\n",
       " 'codes': '862',\n",
       " 'coding': '863',\n",
       " 'coen': '864',\n",
       " 'colchester': '865',\n",
       " 'cold': '866',\n",
       " 'coldcalling': '867',\n",
       " 'collaborating': '868',\n",
       " 'collaboration': '869',\n",
       " 'colleague': '870',\n",
       " 'colleagues': '871',\n",
       " 'collect': '872',\n",
       " 'collected': '873',\n",
       " 'collection': '874',\n",
       " 'collections': '875',\n",
       " 'college': '876',\n",
       " 'colour': '877',\n",
       " 'combat': '878',\n",
       " 'combination': '879',\n",
       " 'combine': '880',\n",
       " 'combined': '881',\n",
       " 'comfort': '882',\n",
       " 'comfortable': '883',\n",
       " 'comforting': '884',\n",
       " 'coming': '885',\n",
       " 'comm': '886',\n",
       " 'command': '887',\n",
       " 'commence': '888',\n",
       " 'commensurate': '889',\n",
       " 'commentary': '890',\n",
       " 'commercial': '891',\n",
       " 'commercially': '892',\n",
       " 'commission': '893',\n",
       " 'commissioning': '894',\n",
       " 'commissions': '895',\n",
       " 'commit': '896',\n",
       " 'commited': '897',\n",
       " 'commitment': '898',\n",
       " 'commitments': '899',\n",
       " 'committed': '900',\n",
       " 'committee': '901',\n",
       " 'commodities': '902',\n",
       " 'commodity': '903',\n",
       " 'common': '904',\n",
       " 'comms': '905',\n",
       " 'communal': '906',\n",
       " 'communicate': '907',\n",
       " 'communicated': '908',\n",
       " 'communicating': '909',\n",
       " 'communication': '910',\n",
       " 'communications': '911',\n",
       " 'communicator': '912',\n",
       " 'community': '913',\n",
       " 'commutable': '914',\n",
       " 'commute': '915',\n",
       " 'companies': '916',\n",
       " 'companionship': '917',\n",
       " \"company's\": '918',\n",
       " 'comparable': '919',\n",
       " 'compass': '920',\n",
       " 'compassionate': '921',\n",
       " 'compatibility': '922',\n",
       " 'compelling': '923',\n",
       " 'compensation': '924',\n",
       " 'competence': '925',\n",
       " 'competencies': '926',\n",
       " 'competency': '927',\n",
       " 'competent': '928',\n",
       " 'competently': '929',\n",
       " 'competition': '930',\n",
       " 'competitive': '931',\n",
       " 'competitor': '932',\n",
       " 'competitors': '933',\n",
       " 'compile': '934',\n",
       " 'compiling': '935',\n",
       " 'complaint': '936',\n",
       " 'complaints': '937',\n",
       " 'complement': '938',\n",
       " 'complete': '939',\n",
       " 'completed': '940',\n",
       " 'completely': '941',\n",
       " 'completeness': '942',\n",
       " 'completing': '943',\n",
       " 'completion': '944',\n",
       " 'complex': '945',\n",
       " 'complexity': '946',\n",
       " 'compliance': '947',\n",
       " 'compliant': '948',\n",
       " 'complied': '949',\n",
       " 'complies': '950',\n",
       " 'comply': '951',\n",
       " 'complying': '952',\n",
       " 'component': '953',\n",
       " 'components': '954',\n",
       " 'composite': '955',\n",
       " 'comprehension': '956',\n",
       " 'comprehensive': '957',\n",
       " 'compressed': '958',\n",
       " 'compressors': '959',\n",
       " 'computational': '960',\n",
       " 'computations': '961',\n",
       " 'computer': '962',\n",
       " 'computerised': '963',\n",
       " 'concentrate': '964',\n",
       " 'concentration': '965',\n",
       " 'concept': '966',\n",
       " 'concepts': '967',\n",
       " 'conceptual': '968',\n",
       " 'concern': '969',\n",
       " 'concerned': '970',\n",
       " 'concerns': '971',\n",
       " 'concise': '972',\n",
       " 'conclusion': '973',\n",
       " 'condition': '974',\n",
       " 'conditioning': '975',\n",
       " 'conditions': '976',\n",
       " 'conduct': '977',\n",
       " 'conducted': '978',\n",
       " 'conducting': '979',\n",
       " 'conduit': '980',\n",
       " 'confederation': '981',\n",
       " 'conference': '982',\n",
       " 'conferences': '983',\n",
       " 'confidence': '984',\n",
       " 'confident': '985',\n",
       " 'confidential': '986',\n",
       " 'confidentiality': '987',\n",
       " 'confidently': '988',\n",
       " 'configuration': '989',\n",
       " 'confirm': '990',\n",
       " 'confirmation': '991',\n",
       " 'confirming': '992',\n",
       " 'conflicting': '993',\n",
       " 'conjunction': '994',\n",
       " 'connection': '995',\n",
       " 'connections': '996',\n",
       " 'connectivity': '997',\n",
       " 'conscious': '998',\n",
       " 'consent': '999',\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab that we read in from previous task\n",
    "dict_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we shall create the TD-IDF vector which acts as the weighted part of this feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(dict_vocab.keys()) # get list of keys and set as vocab\n",
    "tVectoriser =TfidfVectorizer(analyzer= 'word', vocabulary=vocab)\n",
    "tdidf_features = tVectoriser.fit_transform([\"\".join(description) for description in description_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aca:0.09410410296641633 accept:0.10096474809725313 acts:0.09904005135573782 add:0.10420762550977301 addition:0.09133548693324978 agency:0.05910111814497674 analysis:0.1540021949503168 analyst:0.20614602968355752 applying:0.08719435668686773 approval:0.12073605172451188 assist:0.07029983585311665 audit:0.0900711394803754 big:0.118405470963847 call:0.061163979348252503 candidates:0.05447027884691742 capital:0.10096474809725313 cases:0.11260195810861637 central:0.18820820593283266 channel:0.13867387547523674 channels:0.1262951415578527 cima:0.10420762550977301 commercial:0.06931656026853543 completeness:0.13867387547523674 confident:0.0883005901111857 created:0.11437250747780607 cycle:0.11437250747780607 develop:0.1173024062831417 directly:0.0883005901111857 disclaimers:0.10943655908848457 driving:0.07990625751234233 due:0.06955831194709586 employment:0.11912074921712903 endtoend:0.13373792708591523 ensure:0.055734626299791805 expansion:0.09199583622189067 fantastic:0.07106708242186005 feel:0.09563022931371624 finance:0.0773440509869638 financial:0.12134114937822663 firms:0.11629720421932135 forward:0.1431875507235833 found:0.10307301484177876 fp:0.13867387547523674 globally:0.10943655908848457 group:0.06725205230863043 growth:0.07213331572965052 hays:0.21333588611063603 head:0.08417329787489645 input:0.09998147251267189 interest:0.0851351910910539 interested:0.0724081025976423 interpersonal:0.0900711394803754 investment:0.09133548693324978 key:0.055734626299791805 lead:0.07568010451524124 limited:0.0832514953963322 london:0.06496531706957977 market:0.05956037460856452 newly:0.11260195810861637 permanent:0.06307356159761023 plan:0.14708542653127307 planning:0.27630968869129374 policy:0.0851351910910539 presenting:0.11629720421932135 privacy:0.10199377356042202 processes:0.15266148274825397 product:0.06597554095200604 products:0.06381096516117486 proposals:0.10943655908848457 qualified:0.060347648393173 rapid:0.11629720421932135 region:0.09199583622189067 regional:0.09813706645245728 regions:0.1233414193531685 relevant:0.06218508640846879 reporting:0.07029983585311665 retail:0.0866613174383538 revenue:0.09813706645245728 review:0.0791434212832431 specialist:0.06725205230863043 straight:0.13373792708591523 strategic:0.09410410296641633 supply:0.0832514953963322 temporary:0.08193704586447989 thinking:0.0964346837270812 workers:0.0900711394803754 year:0.07383569845502659 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print tdidf weights for first article \n",
    "validator(tdidf_features, vocab, description_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall write the TD-IDF vector to a file and read it in later as it will easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tdidf vectors to text\n",
    "def write_tfidfFile(data_features,filename):\n",
    "    num = data_features.shape[0] # the number of document\n",
    "    out_file = open(filename, 'w') # creates a txt file and open to save the vector representation\n",
    "    for a_ind in range(0, num): # loop through each article by index\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: # for each word index that has non-zero entry in the data_feature\n",
    "            value = data_features[a_ind][0,f_ind] # retrieve the value of the entry from data_features\n",
    "            out_file.write(\"{}:{} \".format(f_ind,value)) # write the entry to the file in the format of word_index:value\n",
    "        out_file.write('\\n') # start a new line after each article\n",
    "    out_file.close() # close the file\n",
    "    \n",
    "write_tfidfFile(tdidf_features,'tVector_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will map each word in the vocab to its respective weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_wordweights(fName_tVectors, voc_dict):\n",
    "    # a list to store the  word:weight dictionaries of documents, each element if a job description which contains dictionary of \n",
    "    # weights for that job.\n",
    "    tfidf_weights = [] \n",
    "    \n",
    "    with open(fName_tVectors) as tVecf: \n",
    "        tVectors = tVecf.read().splitlines() # each line is a tfidf vector representation of a document in string format 'word_index:weight word_index:weight .......'\n",
    "    for tv in tVectors: # for each tfidf document vector\n",
    "        tv = tv.strip()\n",
    "        weights = tv.split(' ') # list of 'word_index:weight' entries\n",
    "        weights = [w.split(':') for w in weights] # change the format of weight to a list of '[word_index,weight]' entries\n",
    "        key_list = list(voc_dict.keys()) # create list of all keys\n",
    "        val_list = list(voc_dict.values()) #create all list of values\n",
    "        wordweight_dict = {} # create dict to store word:weight for each description\n",
    "        for w in weights:\n",
    "            position = val_list.index(w[0]) # get the index position of the word\n",
    "            wordweight_dict[key_list[position]] = w[1] # word at index: weight\n",
    "        tfidf_weights.append(wordweight_dict) # apppend the dict to list\n",
    "    return tfidf_weights\n",
    "\n",
    "tfidf_weights = doc_wordweights('tVector_file.txt', dict_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '0.07383569845502659',\n",
       " 'workers': '0.0900711394803754',\n",
       " 'thinking': '0.0964346837270812',\n",
       " 'temporary': '0.08193704586447989',\n",
       " 'supply': '0.0832514953963322',\n",
       " 'strategic': '0.09410410296641633',\n",
       " 'straight': '0.13373792708591523',\n",
       " 'specialist': '0.06725205230863043',\n",
       " 'review': '0.0791434212832431',\n",
       " 'revenue': '0.09813706645245728',\n",
       " 'retail': '0.0866613174383538',\n",
       " 'reporting': '0.07029983585311665',\n",
       " 'relevant': '0.06218508640846879',\n",
       " 'regions': '0.1233414193531685',\n",
       " 'regional': '0.09813706645245728',\n",
       " 'region': '0.09199583622189067',\n",
       " 'rapid': '0.11629720421932135',\n",
       " 'qualified': '0.060347648393173',\n",
       " 'proposals': '0.10943655908848457',\n",
       " 'products': '0.06381096516117486',\n",
       " 'product': '0.06597554095200604',\n",
       " 'processes': '0.15266148274825397',\n",
       " 'privacy': '0.10199377356042202',\n",
       " 'presenting': '0.11629720421932135',\n",
       " 'policy': '0.0851351910910539',\n",
       " 'planning': '0.27630968869129374',\n",
       " 'plan': '0.14708542653127307',\n",
       " 'permanent': '0.06307356159761023',\n",
       " 'newly': '0.11260195810861637',\n",
       " 'market': '0.05956037460856452',\n",
       " 'london': '0.06496531706957977',\n",
       " 'limited': '0.0832514953963322',\n",
       " 'lead': '0.07568010451524124',\n",
       " 'key': '0.055734626299791805',\n",
       " 'investment': '0.09133548693324978',\n",
       " 'interpersonal': '0.0900711394803754',\n",
       " 'interested': '0.0724081025976423',\n",
       " 'interest': '0.0851351910910539',\n",
       " 'input': '0.09998147251267189',\n",
       " 'head': '0.08417329787489645',\n",
       " 'hays': '0.21333588611063603',\n",
       " 'growth': '0.07213331572965052',\n",
       " 'group': '0.06725205230863043',\n",
       " 'globally': '0.10943655908848457',\n",
       " 'fp': '0.13867387547523674',\n",
       " 'found': '0.10307301484177876',\n",
       " 'forward': '0.1431875507235833',\n",
       " 'firms': '0.11629720421932135',\n",
       " 'financial': '0.12134114937822663',\n",
       " 'finance': '0.0773440509869638',\n",
       " 'feel': '0.09563022931371624',\n",
       " 'fantastic': '0.07106708242186005',\n",
       " 'expansion': '0.09199583622189067',\n",
       " 'ensure': '0.055734626299791805',\n",
       " 'endtoend': '0.13373792708591523',\n",
       " 'employment': '0.11912074921712903',\n",
       " 'due': '0.06955831194709586',\n",
       " 'driving': '0.07990625751234233',\n",
       " 'disclaimers': '0.10943655908848457',\n",
       " 'directly': '0.0883005901111857',\n",
       " 'develop': '0.1173024062831417',\n",
       " 'cycle': '0.11437250747780607',\n",
       " 'created': '0.11437250747780607',\n",
       " 'confident': '0.0883005901111857',\n",
       " 'completeness': '0.13867387547523674',\n",
       " 'commercial': '0.06931656026853543',\n",
       " 'cima': '0.10420762550977301',\n",
       " 'channels': '0.1262951415578527',\n",
       " 'channel': '0.13867387547523674',\n",
       " 'central': '0.18820820593283266',\n",
       " 'cases': '0.11260195810861637',\n",
       " 'capital': '0.10096474809725313',\n",
       " 'candidates': '0.05447027884691742',\n",
       " 'call': '0.061163979348252503',\n",
       " 'big': '0.118405470963847',\n",
       " 'audit': '0.0900711394803754',\n",
       " 'assist': '0.07029983585311665',\n",
       " 'approval': '0.12073605172451188',\n",
       " 'applying': '0.08719435668686773',\n",
       " 'analyst': '0.20614602968355752',\n",
       " 'analysis': '0.1540021949503168',\n",
       " 'agency': '0.05910111814497674',\n",
       " 'addition': '0.09133548693324978',\n",
       " 'add': '0.10420762550977301',\n",
       " 'acts': '0.09904005135573782',\n",
       " 'accept': '0.10096474809725313',\n",
       " 'aca': '0.09410410296641633'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing out the weights of the first description\n",
    "tfidf_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall generate the weight word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_docvecs(embeddings, tfidf, docs):\n",
    "    vecs = np.zeros((len(docs), embeddings.vector_size))\n",
    "    for i, doc in enumerate(docs):\n",
    "        \n",
    "        valid_keys = [term for term in doc if term in embeddings.key_to_index]\n",
    "        tf_weights = [float(tfidf[i].get(term, 0.)) for term in valid_keys]\n",
    "        assert len(valid_keys) == len(tf_weights)\n",
    "        weighted = [embeddings[term] * w for term, w in zip(valid_keys, tf_weights)]\n",
    "        if weighted: # same problem as before, some weighted lists are empty, so we only execute if not empty\n",
    "            docvec = np.vstack(weighted)\n",
    "            docvec = np.sum(docvec, axis=0)\n",
    "            vecs[i,:] = docvec\n",
    "    return vecs\n",
    "\n",
    "#genearte word embeddings\n",
    "weighted_preTW2v_dvs = weighted_docvecs(preTW2v_wv, tfidf_weights, df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00930631, -0.0052348 , -0.01366864, ...,  0.00843384,\n",
       "         0.00282097, -0.00080703],\n",
       "       [-0.00603154, -0.01736742,  0.03233934, ...,  0.01103644,\n",
       "         0.01788075,  0.02164512],\n",
       "       [ 0.00854657,  0.00101005,  0.02610588, ..., -0.00138882,\n",
       "        -0.01157671, -0.0137522 ],\n",
       "       ...,\n",
       "       [ 0.00133563, -0.00392716,  0.0018041 , ..., -0.00721641,\n",
       "         0.00064788, -0.0034886 ],\n",
       "       [ 0.01191708,  0.01309026,  0.01315201, ..., -0.00595854,\n",
       "         0.01376948, -0.03581299],\n",
       "       [ 0.01173777,  0.02166973, -0.00349876, ..., -0.00208797,\n",
       "         0.03009685,  0.03942687]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_preTW2v_dvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "#read in webindex file and append each line as a element in a list\n",
    "webindex_list = []\n",
    "with open('webindex.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = f.read().split('\\n')\n",
    "        webindex_list = line\n",
    "print(len(webindex_list))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function writes count vector to index of word based on vocab text file\n",
    "def write_vectorFile(data_features,filename,webindex):\n",
    "    num = data_features.shape[0] # the number of job descriptions\n",
    "    out_file = open(filename, 'w') \n",
    "    for a_ind in range(0, num): # loop through each description\n",
    "        out_file.write('#'+ webindex[a_ind]) #write out '#' followed by the webindex\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: # only get values that are non zero\n",
    "            value = data_features[a_ind][0,f_ind] # get count value\n",
    "            \n",
    "            # write the vector count to the file in the format of word_index:value\n",
    "            #',' at front to ignore edge cases such as -  if it is the last word count vector\n",
    "            out_file.write(\",{}:{}\".format(f_ind,value)) \n",
    "        out_file.write('\\n') # start a new line after each article\n",
    "    out_file.close() # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vectorFile(count_features,'count_vectors.txt',webindex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 We use the count feature vector representation and unweighted word embeddings to create two classification model in which we shall compare.\n",
    "\n",
    "3.2 We build 2 classification model, one model will just have the features of the job title, the other has both the job title and its description. The third model results are taken from 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Language Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Bag of words Machine Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a string to a vector\n",
    "def str2vec(vec_str,voc_size): # vec_str is a line in the vector txt file, voc_size is the length of the vocab\n",
    "    doc_vec = [0] * voc_size \n",
    "    # processing the vec_str\n",
    "    vec_str_as_list = vec_str.split(',') # splits string based on ',' NOTE** contains the webindex\n",
    "    only_vec = vec_str_as_list[1:] # removes the webindex\n",
    "    for pair in only_vec: # this only contains word_index, freq\n",
    "        w_ind = int(pair.split(':')[0]) # get the first value which is the word index\n",
    "        w_freq = float(pair.split(':')[1]) # get the second value which is the freq\n",
    "        doc_vec[w_ind] = w_freq # dict = {word_index: word_freq}\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes name of vector representaion and vocab size\n",
    "# reads each line of the file to construct a maxtrix representation\n",
    "# converts sparse matrix into CSR matrix\n",
    "def vecF2matrix(vec_fname,voc_size): \n",
    "    with open(vec_fname) as vecf:\n",
    "        vec_strings = vecf.readlines() # reading a list of strings, each for a document/article\n",
    "    doc_vectors = [str2vec(vstr.strip(),voc_size) for vstr in vec_strings] # construct the matrix representation for the corpus                                                                  # by calling the 'str2vec' function for each line/string\n",
    "    return csr_matrix(doc_vectors) # convert the sparse matrix into csr format and return the obtain csr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evalute mode based on score\n",
    "def evaluate(X_train,X_test,y_train, y_test,seed):\n",
    "    model = LogisticRegression(random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in vocab and count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5168\n"
     ]
    }
   ],
   "source": [
    "vocab = 'vocab.txt'\n",
    "with open(vocab) as f:\n",
    "    voc_size = len(f.readlines())\n",
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776, 5168)\n"
     ]
    }
   ],
   "source": [
    "count_features = vecF2matrix('count_vectors.txt',voc_size)\n",
    "print(count_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in Labels and Webindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "776\n"
     ]
    }
   ],
   "source": [
    "#read all labels in list\n",
    "labels_list = []\n",
    "with open('class.txt', 'r') as f:\n",
    "    labels_list = f.read().split('\\n') # read file and split based on newline and assign list as output\n",
    "print('Output:',labels_list[-1]) #edge case where there is no last element b/c from the way we saved the file\n",
    "labels_list.pop(-1) # remove last element\n",
    "print(len(labels_list)) # confirm the it is removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance', 'Accounting_Finance']\n"
     ]
    }
   ],
   "source": [
    "print(labels_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "#read all webindex\n",
    "webindex_list = []\n",
    "with open('webindex.txt', 'r') as f:\n",
    "    webindex_list = f.read().split('\\n')\n",
    "print(webindex_list[-1]) # same problem as label edge case of NULL value for last elment\n",
    "webindex_list.pop(-1)\n",
    "print(len(webindex_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['68802053', '70757636', '71356489', '69073629', '70656648', '68531828', '72451165', '71852020', '71142126', '68700672']\n"
     ]
    }
   ],
   "source": [
    "print(webindex_list[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression model based on bags of words language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset in train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(count_features, labels_list ,test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :  0.8846153846153846\n",
      "Fold 2 :  0.8580645161290322\n",
      "Fold 3 :  0.8580645161290322\n",
      "Fold 4 :  0.8580645161290322\n",
      "Fold 5 :  0.896774193548387\n",
      "Average score: 0.8711166253101735\n"
     ]
    }
   ],
   "source": [
    "#using 5 fold validation on test\n",
    "kf = KFold(n_splits= 5, random_state=seed, shuffle = True) # set up 5 fold validation\n",
    "fold = 0\n",
    "score = []\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_list)))):\n",
    "    y_train = [labels_list[i] for i in train_index] # assign y_train index\n",
    "    y_test = [labels_list[i] for i in test_index] # assign y test index\n",
    "    X_train_count, X_test_count = count_features[train_index], count_features[test_index] #assign respective xtrain and xtest\n",
    "    print('Fold',fold+1,': ',evaluate(count_features[train_index],\n",
    "                                      count_features[test_index],y_train,y_test,seed)) #evaluate each fold\n",
    "    score.append(evaluate(count_features[train_index],count_features[test_index],y_train,y_test,seed))\n",
    "    fold +=1\n",
    "print('Average score:',sum(score)/len(score)) #print average score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the count feature representation we have achieved a average score of 87.11%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Word Embeddings Language Model Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset in train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(tdidf_features, labels_list ,test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :  0.9230769230769231\n",
      "Fold 2 :  0.8838709677419355\n",
      "Fold 3 :  0.8774193548387097\n",
      "Fold 4 :  0.8516129032258064\n",
      "Fold 5 :  0.9096774193548387\n",
      "Average score: 0.8891315136476428\n"
     ]
    }
   ],
   "source": [
    "#using 5 fold validation on test\n",
    "kf = KFold(n_splits= 5, random_state=seed, shuffle = True) # set up 5 fold validation\n",
    "fold = 0\n",
    "score = []\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_list)))):\n",
    "    y_train = [labels_list[i] for i in train_index] # assign y_train index\n",
    "    y_test = [labels_list[i] for i in test_index] # assign y test index\n",
    "    X_train_count, X_test_count = tdidf_features[train_index], tdidf_features[test_index] #assign respective xtrain and xtest\n",
    "    print('Fold',fold+1,': ',evaluate(tdidf_features[train_index],\n",
    "                                      tdidf_features[test_index],y_train,y_test,seed)) #evaluate each fold\n",
    "    score.append(evaluate(tdidf_features[train_index],tdidf_features[test_index],y_train,y_test,seed))\n",
    "    fold +=1\n",
    "print('Average score:',sum(score)/len(score)) #print average score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the unweighted word embedding feature representation we have achieved a average score of 88.91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the unweighted word embeddings model does a little better than the count feature model which a score of 88.91% compared to 87.11%. This could be due to the word embeddings taking into account the context of words. Altought not tried in this task, the the weighted word embeddings could even prove more accurate since it has the additional parameter of weights which could improve the model. Additionally, different word embeddings model could improve the word embedding model. This this task we utilise word2vec, which is not the most accurate model. Models such as GloVe or FastText could perform better than our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 More information, higher accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Just job Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In task 1, the only information that we extracted was the title strings, which we saved into a text file. Each line in the title.txt represents the title of one job. In order to build answer this question we have to read in the title.txt and proceed to do all the text pro-processing steps, similar to task 1. Although there will some modifications to the pro-processings steps since we dont have alot of data to work with, compared to the task 1. Next we will create a feature representation for job title, as this will be needed for our model. The final step will be build a model and obtain a accuracy score which will be compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since title.txt is just list of strings we need to tokenise it\n",
    "def tokeniseData(description):\n",
    "    '''\n",
    "    Function tokenises a description string\n",
    "    '''\n",
    "    lower_description = description.lower() # convert all to lowercase\n",
    "    pattern = r'''(?x)\n",
    "    [a-zA-Z]+(?:[-'][a-zA-Z]+)? # whole words or words with hyphens/ apostrophe\n",
    "    '''\n",
    "    tokenizer = nltk.RegexpTokenizer(pattern) \n",
    "    tokenised_description = tokenizer.tokenize(lower_description)\n",
    "    return tokenised_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last element:  \n",
      "last element:  Title: Estate Agency Senior Sales Negotiator\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "#read in title.txt\n",
    "title_list = []\n",
    "with open('title.txt', 'r') as f:\n",
    "    title_list = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "print('last element: ',title_list[-1]) #check last element as some files save nothing as the last element\n",
    "title_list.pop(-1) # remove last element\n",
    "print('last element: ',title_list[-1]) # check last element again\n",
    "print(len(title_list)) # print total titles which should equal to number of jobs which is 776\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'fp', 'a', 'blue', 'chip']\n"
     ]
    }
   ],
   "source": [
    "#tokenised the list of title strings\n",
    "tokenised_title = [tokeniseData(title) for title in title_list] \n",
    "print(tokenised_title[0]) # check first element of tokenised title to confirm it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there a few tokens in each title, we shall only remove the stop words and the top 25 most frequent words based on document frequency, as even with less data we wont be able to obtain a accurate model.\n",
    "\n",
    "With few tokens in each title, there is high chance that most words will only appear once in the whole document, therefore if we did remove words that only appear once in document, similar to job descrption, we will remove alot of data, which is higly undesirable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in stop words file\n",
    "stopwords = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'fp', 'blue', 'chip']\n"
     ]
    }
   ],
   "source": [
    "# for each title remove the word if it is in the stopword list\n",
    "tokenised_title = [[w for w in title if w not in stopwords] for title in tokenised_title]\n",
    "print(tokenised_title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same element, we see that we have removed the stop word 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'fp', 'blue', 'chip']\n"
     ]
    }
   ],
   "source": [
    "# removing words that only contain one character\n",
    "tokenised_title= [[w for w in title if len(w)  >= 2 ] for title in tokenised_title] \n",
    "print(tokenised_title[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'manager', 'sales', 'engineer', 'senior', 'nurse', 'executive', 'assistant', 'care', 'rgn', 'business', 'account', 'support', 'analyst', 'consultant', 'london', 'design', 'development', 'home', 'accountant', 'commercial', 'field', 'finance', 'electrical', 'financial']\n"
     ]
    }
   ],
   "source": [
    "#get the 3 most common words based on document frequency\n",
    "words = list(chain.from_iterable([set(title) \\\n",
    "                                    for title in tokenised_title])) # get set of unique words for that article\n",
    "doc_freq = FreqDist(words)\n",
    "most_freq_doc = []\n",
    "freq_doc = doc_freq.most_common(25) #output : list of tuple (word,freq)\n",
    "#append 50 most common words to a list\n",
    "for i in freq_doc:\n",
    "    most_freq_doc.append(i[0])\n",
    "print(most_freq_doc)#check the list if they contain the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word 'title', 'manager' and 'sales' are top 3 most common words based on document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fp', 'blue', 'chip']\n"
     ]
    }
   ],
   "source": [
    "#for each title remove the 25 most common words based on document frequency if they exist in the title\n",
    "tokenised_title = [[w for w in title if w not in most_freq_doc] for title in tokenised_title]\n",
    "print(tokenised_title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first title tokens we see that the token title has been removed.\n",
    "\n",
    "In order to compare without any bias, the features generated will have to be similar, therefore we will build a count vector for the title tokens similar to what we did with the job description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbey', 'aberdeen', 'abi', 'accepted', 'accounant', 'accountancy', 'accounting', 'accounts', 'actions', 'activities']\n"
     ]
    }
   ],
   "source": [
    "#generate vocab based on title tokens\n",
    "words_final = list(chain.from_iterable(tokenised_title)) #flatten the tokenised words of descriptions\n",
    "vocab_title = sorted(set(words_final))\n",
    "print(vocab_title[0:10]) # sample of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776, 930)\n"
     ]
    }
   ],
   "source": [
    "#generate count features for title\n",
    "cVectoriser =CountVectorizer(analyzer= 'word', vocabulary=vocab_title)\n",
    "\n",
    "#creates a string of tokens(job description, and for each string fit to count_vectorizer\n",
    "title_count_features = cVectoriser.fit_transform([\" \".join(title) for title in tokenised_title])\n",
    "print(title_count_features.shape) # (number of job title, size of vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue:1 chip:1 fp:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#validate the count features buy printing first count for first job title\n",
    "validator(title_count_features, vocab_title, tokenised_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(title_count_features, labels_list ,test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :  0.6858974358974359\n",
      "Fold 2 :  0.6967741935483871\n",
      "Fold 3 :  0.7032258064516129\n",
      "Fold 4 :  0.6645161290322581\n",
      "Fold 5 :  0.7096774193548387\n",
      "Average score: 0.6920181968569066\n"
     ]
    }
   ],
   "source": [
    "#using 5 fold validation on test\n",
    "kf = KFold(n_splits= 5, random_state=seed, shuffle = True) # set up 5 fold validation\n",
    "fold = 0\n",
    "score = []\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_list)))):\n",
    "    y_train = [labels_list[i] for i in train_index] # assign y_train index\n",
    "    y_test = [labels_list[i] for i in test_index] # assign y test index\n",
    "    X_train_count, X_test_count = title_count_features[train_index], title_count_features[test_index] #assign respective xtrain and xtest\n",
    "    print('Fold',fold+1,': ',evaluate(title_count_features[train_index],\n",
    "                                      title_count_features[test_index],y_train,y_test,seed)) #evaluate each fold\n",
    "    score.append(evaluate(title_count_features[train_index],title_count_features[test_index],y_train,y_test,seed))\n",
    "    fold +=1\n",
    "print('Average score:',sum(score)/len(score)) #print average score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy score for the classification model on just using the title as input data is 69.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Just job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text pro-processing was done in task 1. Feature generations on the pro-processed text was obtained during task 2. The classifcation modelling on just the description data was done during the first half of task 3. The results are repeated below for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :  0.8846153846153846\n",
      "Fold 2 :  0.8580645161290322\n",
      "Fold 3 :  0.8580645161290322\n",
      "Fold 4 :  0.8580645161290322\n",
      "Fold 5 :  0.896774193548387\n",
      "Average score: 0.8711166253101735\n"
     ]
    }
   ],
   "source": [
    "#using 5 fold validation on test\n",
    "kf = KFold(n_splits= 5, random_state=seed, shuffle = True) # set up 5 fold validation\n",
    "fold = 0\n",
    "score = []\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_list)))):\n",
    "    y_train = [labels_list[i] for i in train_index] # assign y_train index\n",
    "    y_test = [labels_list[i] for i in test_index] # assign y test index\n",
    "    X_train_count, X_test_count = count_features[train_index], count_features[test_index] #assign respective xtrain and xtest\n",
    "    print('Fold',fold+1,': ',evaluate(count_features[train_index],\n",
    "                                      count_features[test_index],y_train,y_test,seed)) #evaluate each fold\n",
    "    score.append(evaluate(count_features[train_index],count_features[test_index],y_train,y_test,seed))\n",
    "    fold +=1\n",
    "print('Average score:',sum(score)/len(score)) #print average score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy score for the classification model on just using the title as input data is 87.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Both Title and Job Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a model on both the texts of title and job description we will concatenate the tokens of the title with the tokens of the job description. They way we have saved or extract tokens means the first element in the title tokens correlates to the first element in the description tokens. No text pro-processing is required since we already have done it previously, but we have to build a new vocabulary. The next step would be to generate a count vector feature for consistency, where we will finally use the feature to build a classification model and gague the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first print the first element of each list to see the structure of the list. We shall also print the length of each list to ensure they are both the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fp', 'blue', 'chip']\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "#printing \n",
    "print(tokenised_title[0])\n",
    "print(len(tokenised_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['market', ' retail', ' rapid', ' growth', ' due', ' expansion', ' add', ' financial', ' planning', ' analyst', ' central', ' central', ' london', ' fantastic', ' newly', ' created', ' driving', ' forward', ' financial', ' planning', ' analysis', ' group', ' reporting', ' directly', ' head', ' fp', ' assist', ' revenue', ' analysis', ' product', ' channel', ' region', ' commercial', ' input', ' review', ' cases', ' presenting', ' proposals', ' approval', ' develop', ' endtoend', ' planning', ' cycle', ' processes', ' lead', ' regional', ' planning', ' processes', ' ensure', ' completeness', ' key', ' channels', ' products', ' finance', ' year', ' strategic', ' plan', ' addition', ' globally', ' regions', ' develop', ' capital', ' investment', ' plan', ' forward', ' thinking', ' confident', ' qualified', ' analyst', ' aca', ' cima', ' interpersonal', ' interest', ' feel', ' relevant', ' call', ' candidates', ' straight', ' big', ' audit', ' firms', ' interested', ' hays', ' specialist', ' limited', ' acts', ' employment', ' agency', ' permanent', ' employment', ' supply', ' temporary', ' workers', ' applying', ' accept', ' privacy', ' policy', ' disclaimers', ' found', ' hays']\n",
      "776\n"
     ]
    }
   ],
   "source": [
    "print(description_tokens[0])\n",
    "print(len(description_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  776\n"
     ]
    }
   ],
   "source": [
    "#concat the the tokens elementwise via zip function\n",
    "t_and_d = []\n",
    "for title, description in zip(tokenised_title, description_tokens):\n",
    "    concat = title + description\n",
    "    t_and_d.append(concat)\n",
    "print('Total: ',len(t_and_d))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fp', 'blue', 'chip', 'market', ' retail', ' rapid', ' growth', ' due', ' expansion', ' add', ' financial', ' planning', ' analyst', ' central', ' central', ' london', ' fantastic', ' newly', ' created', ' driving', ' forward', ' financial', ' planning', ' analysis', ' group', ' reporting', ' directly', ' head', ' fp', ' assist', ' revenue', ' analysis', ' product', ' channel', ' region', ' commercial', ' input', ' review', ' cases', ' presenting', ' proposals', ' approval', ' develop', ' endtoend', ' planning', ' cycle', ' processes', ' lead', ' regional', ' planning', ' processes', ' ensure', ' completeness', ' key', ' channels', ' products', ' finance', ' year', ' strategic', ' plan', ' addition', ' globally', ' regions', ' develop', ' capital', ' investment', ' plan', ' forward', ' thinking', ' confident', ' qualified', ' analyst', ' aca', ' cima', ' interpersonal', ' interest', ' feel', ' relevant', ' call', ' candidates', ' straight', ' big', ' audit', ' firms', ' interested', ' hays', ' specialist', ' limited', ' acts', ' employment', ' agency', ' permanent', ' employment', ' supply', ' temporary', ' workers', ' applying', ' accept', ' privacy', ' policy', ' disclaimers', ' found', ' hays']\n"
     ]
    }
   ],
   "source": [
    "#print first element\n",
    "print(t_and_d[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first concatenated title and description tokens the format for the tokens are in consistent with some words containing white space while others do not. We shall fix this to keep it consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fp', 'blue', 'chip', 'market', 'retail', 'rapid', 'growth', 'due', 'expansion', 'add', 'financial', 'planning', 'analyst', 'central', 'central', 'london', 'fantastic', 'newly', 'created', 'driving', 'forward', 'financial', 'planning', 'analysis', 'group', 'reporting', 'directly', 'head', 'fp', 'assist', 'revenue', 'analysis', 'product', 'channel', 'region', 'commercial', 'input', 'review', 'cases', 'presenting', 'proposals', 'approval', 'develop', 'endtoend', 'planning', 'cycle', 'processes', 'lead', 'regional', 'planning', 'processes', 'ensure', 'completeness', 'key', 'channels', 'products', 'finance', 'year', 'strategic', 'plan', 'addition', 'globally', 'regions', 'develop', 'capital', 'investment', 'plan', 'forward', 'thinking', 'confident', 'qualified', 'analyst', 'aca', 'cima', 'interpersonal', 'interest', 'feel', 'relevant', 'call', 'candidates', 'straight', 'big', 'audit', 'firms', 'interested', 'hays', 'specialist', 'limited', 'acts', 'employment', 'agency', 'permanent', 'employment', 'supply', 'temporary', 'workers', 'applying', 'accept', 'privacy', 'policy', 'disclaimers', 'found', 'hays']\n"
     ]
    }
   ],
   "source": [
    "t_and_d = [[w.strip() for w in tokens] for tokens in t_and_d] # strip whitespace\n",
    "print(t_and_d[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have strip all whitespace in each token, to ensure the same format for each token. Now we can start pre-processing the tokens an build a meaningful vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aap', 'aaron', 'aat', 'abb', 'abbey', 'abenefit', 'aberdeen', 'abi', 'abilities', 'abreast']\n",
      "5322\n"
     ]
    }
   ],
   "source": [
    "#generate vocab \n",
    "words_final = list(chain.from_iterable(t_and_d)) #flatten the tokenised words of descriptions\n",
    "vocab_t_and_d= sorted(set(words_final))\n",
    "print(vocab_t_and_d[0:10]) # sample of vocab\n",
    "print(len(vocab_t_and_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this vocab is much larger than the two other vocab we have built on just title and just description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776, 5322)\n"
     ]
    }
   ],
   "source": [
    "cVectoriser =CountVectorizer(analyzer= 'word', vocabulary=vocab_t_and_d)\n",
    "\n",
    "#creates a string of tokens(job description, and for each string fit to count_vectorizer\n",
    "t_and_d_count_features = cVectoriser.fit_transform([\" \".join(token) for token in t_and_d])\n",
    "print(t_and_d_count_features.shape) # (number of job title, size of vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aca:1 accept:1 acts:1 add:1 addition:1 agency:1 analysis:2 analyst:2 applying:1 approval:1 assist:1 audit:1 big:1 blue:1 call:1 candidates:1 capital:1 cases:1 central:2 channel:1 channels:1 chip:1 cima:1 commercial:1 completeness:1 confident:1 created:1 cycle:1 develop:2 directly:1 disclaimers:1 driving:1 due:1 employment:2 endtoend:1 ensure:1 expansion:1 fantastic:1 feel:1 finance:1 financial:2 firms:1 forward:2 found:1 fp:2 globally:1 group:1 growth:1 hays:2 head:1 input:1 interest:1 interested:1 interpersonal:1 investment:1 key:1 lead:1 limited:1 london:1 market:1 newly:1 permanent:1 plan:2 planning:4 policy:1 presenting:1 privacy:1 processes:2 product:1 products:1 proposals:1 qualified:1 rapid:1 region:1 regional:1 regions:1 relevant:1 reporting:1 retail:1 revenue:1 review:1 specialist:1 straight:1 strategic:1 supply:1 temporary:1 thinking:1 workers:1 year:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate the count feature generation\n",
    "validator(t_and_d_count_features, vocab_t_and_d, t_and_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(t_and_d_count_features, labels_list ,test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 :  0.9038461538461539\n",
      "Fold 2 :  0.8709677419354839\n",
      "Fold 3 :  0.8838709677419355\n",
      "Fold 4 :  0.8580645161290322\n",
      "Fold 5 :  0.9161290322580645\n",
      "Average score: 0.8865756823821339\n"
     ]
    }
   ],
   "source": [
    "#using 5 fold validation on test\n",
    "kf = KFold(n_splits= 5, random_state=seed, shuffle = True) # set up 5 fold validation\n",
    "fold = 0\n",
    "score = []\n",
    "for train_index, test_index in kf.split(list(range(0,len(labels_list)))):\n",
    "    y_train = [labels_list[i] for i in train_index] # assign y_train index\n",
    "    y_test = [labels_list[i] for i in test_index] # assign y test index\n",
    "    X_train_count, X_test_count = t_and_d_count_features[train_index], title_count_features[test_index] #assign respective xtrain and xtest\n",
    "    print('Fold',fold+1,': ',evaluate(t_and_d_count_features[train_index],\n",
    "                                      t_and_d_count_features[test_index],y_train,y_test,seed)) #evaluate each fold\n",
    "    score.append(evaluate(t_and_d_count_features[train_index],t_and_d_count_features[test_index],y_train,y_test,seed))\n",
    "    fold +=1\n",
    "print('Average score:',sum(score)/len(score)) #print average score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average accuracy score for the classification model on just using the title as input data is 88.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conlusions, the results above suggests that the more information provided, to create a feature representation, will improve the model when classify which class the job will belong to. Further can be done, where we could even include company as well as title and job description. We could also use different feature representation, such as word embeddings as this one utilises count vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment tasks here.\n",
    "In summary, the word embeddings features provided a more accurate model than the bag of word model(count vector), which could be due that it takes in the context of the words. Further testing on different word embeddings language model to generate the features could provide a more accurate model. Additionally, the more information that is provided in order to generate a feature representation, suggests that it will always result in a more accurate model, which makes sense as the more data that is availiable, the more accurate the model will be.\n",
    "\n",
    "***NOTE ENSURE THAT ALL REQUIRED TEXT FILES ARE IN THE SAME DIRECTORY AS THIS NOTEBOOK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
